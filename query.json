[
    {
        "id": "0",
        "raw_data": "# 1 Introduction  \n\nLarge-scale language model pretraining has become increasingly prevalent for achieving high performance on a variety of natural language processing (NLP) tasks. When applying these models to a specific task, they are usually fine-tuned using supervised learning, often to maximize the log probability of a set of human demonstrations.  \n\nWhile this strategy has led to markedly improved performance, there is still a misalignment between this fine-tuning objective—-maximizing the likelihood of human-written text—and what we care about—generating high-quality outputs as determined by humans. This misalignment has several causes: the maximum likelihood objective has no distinction between important errors (e.g. making up facts [41]) and unimportant errors (e.g. selecting the precise word from a set of synonyms); models are incentivized to place probability mass on all human demonstrations, including those that are low-quality; and distributional shift during sampling can degrade performance [56, 52]. Quality can often be improved significantly by non-uniform sampling strategies such as beam search [51], but these can lead to repetition and other undesirable artifacts [69, 23]. Optimizing for quality may be a principled approach to overcoming these problems.  \n\n  \nFigure 1: Fraction of the time humans prefer our models? summaries over the human-generated reference summaries on the TL;DR dataset.4Since quality judgments involve an arbitrary decision about how to trade off summary length vs. coverage within the 24-48 token limit, we also provide length-controlled graphs in Appendix F; length differences explain about a third of the gap between feedback and supervised learning at 6.7B.  \n\nOur goal in this paper is to advance methods for training language models on objectives that more closely capture the behavior we care about. To make short-term progress towards this goal, we ",
        "file_name": "2009.01325v3.pdf",
        "CoTQuery": "嗯，用户让我从一段学术文本中提取主要问题和答案，然后根据标注任务的要求设计符合条件的问题。首先，我需要仔细阅读并理解这段文本的内容。这段文字讲的是大规模语言模型预训练在自然语言处理任务中的应用，但指出微调目标（最大化人类撰写文本的似然）与实际目标（生成高质量输出）之间的不一致性。\n\n接下来，我要确定文本的关键点。比如，微调目标的问题，包括不区分重要错误和不重要错误、低质量样本的影响、分布偏移的影响，以及优化质量的可能解决方案。然后，用户需要的问题必须基于这些内容，答案明确存在，覆盖核心信息，清晰简洁，不重复。\n\n现在，我需要逐段分析。引言部分提到大规模预训练的普遍性，微调方法的问题，以及目标不匹配的原因。然后，Figure 1提到人类偏好实验结果，附录中的长度控制图表。最后，研究目标是改进训练方法，以更接近实际目标。\n\n根据这些，我需要设计问题。首先，目标不匹配的原因，列出三个原因。然后，非均匀采样策略如beam search的作用及问题。接着，质量优化可能解决的问题。人类偏好的实验结果，长度差异的影响，研究的主要目标，以及错误分类的例子。\n\n要确保每个问题独立，覆盖关键点，答案在文中明确。比如，问不匹配的原因有三个点，答案在第二段。再比如，非均匀采样策略的优缺点，答案在第二段末尾。需要检查问题是否重复，是否符合要求。可能需要生成大约六个问题，确保覆盖所有核心内容，不遗漏重要信息。\n\n\n1.Query: 大规模语言模型微调时使用的监督学习目标是什么？  \n2.Query: 文本指出的最大似然目标与期望目标之间存在哪三种不匹配原因？  \n3.Query: 非均匀采样策略如beam search在提升输出质量时可能引发什么问题？  \n4.Query: 根据Figure 1，人类在TL;DR数据集上偏好模型生成的摘要还是人工参考摘要？  \n5.Query: 长度差异对反馈学习与监督学习的性能差距贡献比例是多少？  \n6.Query: 文中提到的优化质量目标可能解决哪些核心问题？  \n7.Query: 文本中列举的\"重要错误\"和\"不重要错误\"具体指什么？  \n8.Query: 模型在微调过程中为何会被激励关注低质量的人类示范样本？  \n9.Query: 分布偏移现象在哪些文献中被提及会导致性能下降？  \n10.Query: 本论文的核心研究目标是什么？"
    }
]

